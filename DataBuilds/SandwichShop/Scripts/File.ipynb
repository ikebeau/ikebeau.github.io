{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & Create Departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Departments = ['Chips', 'Bread', 'Sweets', 'Meat', 'Fruit', 'Vegetables', 'Sandwiches', 'Salads', 'Drinks']\n",
    "Class = {\n",
    "        # Walmart\n",
    "         'Chips': ['Potato', 'Tortilla', 'Vegetable'],\n",
    "        # Great Harvest\n",
    "         'Bread': ['Leanvened', 'Flatbreads', 'Specialty'],\n",
    "        # CheeseCake Factory \n",
    "         'Sweets': ['Baked', 'Confections', 'Frozen'],\n",
    "        # 56cutz\n",
    "         'Meat': ['Red', 'Poultry', 'Seafood'],\n",
    "        # Whole Food Market \n",
    "         'Fruit': ['Citrus', 'Stone', 'Berries', 'Pome', 'Tropical'],\n",
    "        # Whole Foods Market\n",
    "         'Vegetables': ['Leafy', 'Root', 'Cruciferous', 'Fruiting'],\n",
    "         # Subway\n",
    "         'Sandwiches': ['Cold', 'Hot', 'Open-Faced'],\n",
    "         # Cafe Zupas\n",
    "         'Salads': ['Green', 'Grain & Pasta', 'Protein-Based'],\n",
    "         # Jamba Juice\n",
    "         'Drinks': ['Smoothies', 'Soft', 'Juices']}\n",
    "Fineline = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walmart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great Harvest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheese Cake Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 56cutz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole Food Market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## East Coast Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "url = 'https://eastcoastsubsmurray.com/menu' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Hot Subs\n",
      "Category: Cold Subs\n",
      "Category: Combo Meals\n",
      "Category: Tasty Sides\n",
      "Category: Kid's Menu\n",
      "Category: Salads\n",
      "Category: Drinks\n",
      "Category: Party Subs\n",
      "Category: Party Trays & Platters\n"
     ]
    }
   ],
   "source": [
    "driver.get(url)\n",
    "\n",
    "# Allow time for the page to fully load\n",
    "time.sleep(5)  # Adjust depending on page load time\n",
    "\n",
    "# Get page source and parse with BeautifulSoup\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Close the driver as we don't need it anymore\n",
    "driver.quit()\n",
    "\n",
    "# Scrape categories (h3 elements)\n",
    "categories = soup.find_all('h3', {'role': 'heading', 'aria-level': '3'})\n",
    "\n",
    "# Loop through each category\n",
    "for category in categories:\n",
    "    category_name = category.get_text(strip=True)\n",
    "    print(f\"Category: {category_name}\")\n",
    "    \n",
    "    # Find the next div or element containing the items related to the current category\n",
    "    category_items_div = category.find_next('div')\n",
    "    \n",
    "    # If the category items div exists, proceed\n",
    "    if category_items_div:\n",
    "        # Find all divs within this section that contain the item and price info\n",
    "        item_divs = category_items_div.find_all('div', recursive=True)\n",
    "\n",
    "        # Loop through each item div\n",
    "        for item_div in item_divs:\n",
    "            # Extract the item name (e.g., h4 element or similar, adjust if necessary)\n",
    "            item_name = item_div.find('h4')  # Adjust to correct tag for item name\n",
    "            # Extract the price (div with specific class or other identifiers)\n",
    "            price = item_div.find('div', class_='PriceMajor')  # Adjust class name as needed\n",
    "\n",
    "            # If both item name and price are found, print them\n",
    "            if item_name and price:\n",
    "                print(f\"  Item: {item_name.get_text(strip=True)}\")\n",
    "                print(f\"  Price: {price.get_text(strip=True)}\")\n",
    "    else:\n",
    "        print(f\"No items found for category: {category_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ten Seconds Yunnan Rice Noodle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "link = \"https://order.tensecondsricenoodle.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Set up the Selenium WebDriver (make sure to use the appropriate driver for your browser)  # or webdriver.Firefox(), etc.\n",
    "driver.get(link)  # Replace with the actual URL\n",
    "\n",
    "# Give the page some time to load\n",
    "time.sleep(5)  # Adjust sleep time based on the page load speed\n",
    "\n",
    "# Find the menu section (you might need to adjust the selector based on the actual structure of the page)\n",
    "menu_section = driver.find_element(By.CLASS_NAME, 'wrap_middle')\n",
    "tensecond = []\n",
    "# Get the HTML of the menu section\n",
    "html = menu_section.get_attribute('outerHTML')\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Use Beautiful Soup to parse the HTML\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Find the category name\n",
    "category_name = soup.find('div', class_='wrap_category_main').text.strip()\n",
    "\n",
    "# Find all menu items in the section\n",
    "menu_items = soup.find_all('div', class_='wrap_menu_one')\n",
    "\n",
    "# Loop through menu items and extract the name and price\n",
    "for item in menu_items:\n",
    "    item_name = item.find('div', class_='menuName').text.strip()\n",
    "    item_price = item.find('div', class_='menuPrice').text.strip()\n",
    "    tensecond.append({\n",
    "        \"Category\": None,\n",
    "        \"Item\": item_name,\n",
    "        \"Price\": item_price,\n",
    "        \"Calories\": None\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tropical Smoothie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "link = 'https://order.tropicalsmoothiecafe.com/menu/ut-013'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (67, 4)\n",
      "┌───────────────────┬─────────────────────────────┬───────┬──────────┐\n",
      "│ Category          ┆ Item                        ┆ Price ┆ Calories │\n",
      "│ ---               ┆ ---                         ┆ ---   ┆ ---      │\n",
      "│ str               ┆ str                         ┆ f64   ┆ str      │\n",
      "╞═══════════════════╪═════════════════════════════╪═══════╪══════════╡\n",
      "│ FEATURED PRODUCTS ┆ BAHAMA MAMA™ BOWL           ┆ 10.49 ┆ 380      │\n",
      "│ FEATURED PRODUCTS ┆ PB PROTEIN CRUNCH           ┆ 9.19  ┆ 800      │\n",
      "│ FEATURED PRODUCTS ┆ ISLAND PUNCH SMOOTHIE       ┆ 8.69  ┆ 470      │\n",
      "│ BREAKFAST         ┆ SPINACH FETA & PESTO WRAP   ┆ 5.69  ┆ 450      │\n",
      "│ …                 ┆ …                           ┆ …     ┆ …        │\n",
      "│ BOTTLED BEVERAGES ┆ GOLD PEAK® SWEET TEA        ┆ 2.69  ┆ 190      │\n",
      "│ BOTTLED BEVERAGES ┆ COCA-COLA® 20 oz            ┆ 2.69  ┆ 240      │\n",
      "│ BOTTLED BEVERAGES ┆ COCA-COLA® ZERO SUGAR 20 oz ┆ 2.69  ┆ 0        │\n",
      "│ BOTTLED BEVERAGES ┆ SPRITE® 20 oz               ┆ 2.69  ┆ 230      │\n",
      "└───────────────────┴─────────────────────────────┴───────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Accept Cookies\n",
    "try:\n",
    "    accept_button = driver.find_element(By.ID, \"onetrust-accept-btn-handler\")\n",
    "    accept_button.click()\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "# Close Menu\n",
    "try:\n",
    "    close_button = driver.find_element(By.CLASS_NAME, \"_closeButton_1hgyd2\")\n",
    "    close_button.click()\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "# Initialize a list to store the scraped data\n",
    "data = []\n",
    "\n",
    "# Find all section elements that contain product categories\n",
    "sections = driver.find_elements(By.CSS_SELECTOR, 'section[data-test-menucategory-carousel]')\n",
    "\n",
    "# Loop through each section to extract category and items\n",
    "for section in sections:\n",
    "    # Extract the category title (h2)\n",
    "    try:\n",
    "        category_title = section.find_element(By.TAG_NAME, 'h2').text\n",
    "    except NoSuchElementException:\n",
    "        category_title = \"No category\"\n",
    "\n",
    "    # Find the list of items (inside 'ul' tag)\n",
    "    try:\n",
    "        item_list = section.find_element(By.TAG_NAME, 'ul')\n",
    "        items = item_list.find_elements(By.TAG_NAME, 'li')\n",
    "\n",
    "        for item in items:\n",
    "            # Extract item name\n",
    "            try:\n",
    "                item_name = item.find_element(By.CSS_SELECTOR, 'h3._name_1u8ugx a').text\n",
    "            except NoSuchElementException:\n",
    "                item_name = None\n",
    "            \n",
    "            # Extract item price\n",
    "            try:\n",
    "                item_price = item.find_element(By.CSS_SELECTOR, 'p._price_1u8ugx').text\n",
    "            except NoSuchElementException:\n",
    "                item_price = None\n",
    "\n",
    "            # Extract item calories\n",
    "            try:\n",
    "                item_calories = item.find_element(By.CSS_SELECTOR, 'p._calories_1u8ugx').text\n",
    "            except NoSuchElementException:\n",
    "                item_calories = None\n",
    "\n",
    "            # Append the data to the list\n",
    "            data.append({\n",
    "                \"Category\": category_title,\n",
    "                \"Item\": item_name,\n",
    "                \"Price\": item_price,\n",
    "                \"Calories\": item_calories\n",
    "            })\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        continue\n",
    "\n",
    "# Convert the data to a Polars DataFrame\n",
    "df = pl.DataFrame(data).filter(pl.col(\"Price\").is_not_null())\\\n",
    "                        .with_columns(pl.col(\"Price\").str.replace(r'[$]','').cast(pl.Float64),\n",
    "                                      pl.col('Calories').str.replace(' Calories', ''))\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
